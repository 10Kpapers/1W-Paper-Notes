# 一万篇论文笔记计划
[[1] Blockwise Parallel Decoding: 一种加速LLM解码的并行方法](https://zhuanlan.zhihu.com/p/652823496)

[[2] Contrastive Decoding: 一种可提高文本生成质量的解码方法](https://zhuanlan.zhihu.com/p/653387559)

[[3] contrastive search: 一种提高文本生成质量的解码方法](https://zhuanlan.zhihu.com/p/653450352)

[[4] LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://zhuanlan.zhihu.com/p/653964820)

[[5] SimVLM: 图片和文本拼接成prompt做Language Model训练](https://zhuanlan.zhihu.com/p/654050155)

[[6] CoCa: 在图生文过程中加入对比学习](https://zhuanlan.zhihu.com/p/654161055)

[[7] DiffSTG: 将扩散模型用于处理时空数据](https://zhuanlan.zhihu.com/p/656236507)

[[8] MolCLR: 基于GNN 的分子图表征对比学习框架](https://zhuanlan.zhihu.com/p/656769732)

[[9] CLIP4Clip: 利用 CLIP 做视频-文本检索](https://zhuanlan.zhihu.com/p/656811444)

[[10] CenterCLIP: 利用聚类算法提高文本-视频检索的效率](https://zhuanlan.zhihu.com/p/656811473)

[[11] Scaling Laws for Neural Machine Translation](https://zhuanlan.zhihu.com/p/656888149)

[[12] DocRED: 一个大规模文档级关系抽取数据集](https://zhuanlan.zhihu.com/p/656964531)

[[13] DocuNet: 把文档级关系抽取看作语义分割任务](https://zhuanlan.zhihu.com/p/656975670)

[[14] Segmenter: 用Transformer 做语义分割](https://zhuanlan.zhihu.com/p/657294681)

[[15] Swin Transformer: 一种层级 ViT 模型](https://zhuanlan.zhihu.com/p/657359846)

[[16] VALOR: Vision-Audio-Language三模态模型和数据集](https://zhuanlan.zhihu.com/p/657414556)

[[17] Multimodal Transformer: 多模态的院内死亡风险预测模型](https://zhuanlan.zhihu.com/p/657414912)

[[18] HiTANet: 层级Time-Aware Attention Networks做疾病风险预测](https://zhuanlan.zhihu.com/p/657543094)

[[19] TranSalNet: 融合 CNN+Transformer 做视觉显著性预测](https://zhuanlan.zhihu.com/p/657415194)

[[20] TransUNet: 将Transformer Encoder 融入 U-Net 做医学图像分割](https://zhuanlan.zhihu.com/p/657901979)

[[21] Swin-Unet: 只用Swin Transformer构建Unet 结构做医学图像分割](https://zhuanlan.zhihu.com/p/657910667)

[[22] iGPT: Generative Pretraining from Pixels](https://zhuanlan.zhihu.com/p/657963444)

