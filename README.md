# 一万篇论文笔记计划
[[1] Blockwise Parallel Decoding: 一种加速LLM解码的并行方法](https://zhuanlan.zhihu.com/p/652823496)

[[2] Contrastive Decoding: 一种可提高文本生成质量的解码方法](https://zhuanlan.zhihu.com/p/653387559)

[[3] contrastive search: 一种提高文本生成质量的解码方法](https://zhuanlan.zhihu.com/p/653450352)

[[4] LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://zhuanlan.zhihu.com/p/653964820)

[[5] SimVLM: 图片和文本拼接成prompt做Language Model训练](https://zhuanlan.zhihu.com/p/654050155)

[[6] CoCa: 在图生文过程中加入对比学习](https://zhuanlan.zhihu.com/p/654161055)

[[7] DiffSTG: 将扩散模型用于处理时空数据](https://zhuanlan.zhihu.com/p/656236507)

[[8] MolCLR: 基于GNN 的分子图表征对比学习框架](https://zhuanlan.zhihu.com/p/656769732)

[[9] CLIP4Clip: 利用 CLIP 做视频-文本检索](https://zhuanlan.zhihu.com/p/656811444)

[[10] CenterCLIP: 利用聚类算法提高文本-视频检索的效率](https://zhuanlan.zhihu.com/p/656811473)

[[11] Scaling Laws for Neural Machine Translation](https://zhuanlan.zhihu.com/p/656888149)

[[12] DocRED: 一个大规模文档级关系抽取数据集](https://zhuanlan.zhihu.com/p/656964531)

[[13] DocuNet: 把文档级关系抽取看作语义分割任务](https://zhuanlan.zhihu.com/p/656975670)

[[14] Segmenter: 用Transformer 做语义分割](https://zhuanlan.zhihu.com/p/657294681)

[[15] Swin Transformer: 一种层级 ViT 模型](https://zhuanlan.zhihu.com/p/657359846)

[[16] VALOR: Vision-Audio-Language三模态模型和数据集](https://zhuanlan.zhihu.com/p/657414556)

[[17] Multimodal Transformer: 多模态的院内死亡风险预测模型](https://zhuanlan.zhihu.com/p/657414912)

[[18] HiTANet: 层级Time-Aware Attention Networks做疾病风险预测](https://zhuanlan.zhihu.com/p/657543094)

[[19] TranSalNet: 融合 CNN+Transformer 做视觉显著性预测](https://zhuanlan.zhihu.com/p/657415194)

[[20] TransUNet: 将Transformer Encoder 融入 U-Net 做医学图像分割](https://zhuanlan.zhihu.com/p/657901979)

[[21] Swin-Unet: 只用Swin Transformer构建Unet 结构做医学图像分割](https://zhuanlan.zhihu.com/p/657910667)

[[22] iGPT: Generative Pretraining from Pixels](https://zhuanlan.zhihu.com/p/657963444)

[[23] 如何对 CLIP 进行 fine-tuning？](https://zhuanlan.zhihu.com/p/658645438)

[[24] ELMo: 生不逢时的动态词向量](https://zhuanlan.zhihu.com/p/658709277)

[[25] DeepWalk: Random Walk + word2vec 学习图节点向量表示](https://zhuanlan.zhihu.com/p/658938347)

[[26] Science BERT: 使用论文数据预训练的 BERT 模型](https://zhuanlan.zhihu.com/p/659205938)

[[27] BioBERT: 生物医学领域自己的 BERT 模型](https://zhuanlan.zhihu.com/p/659209593)

[[28] Clinical BERT: 使用 MIMIC 数据集继续训练 BERT 模型](https://zhuanlan.zhihu.com/p/659607514)

[[29] 不要停，继续预训练](https://zhuanlan.zhihu.com/p/659609320)

[[30] node2vec=skip-gram+bfs+dfs](https://zhuanlan.zhihu.com/p/658731335)

[[31] Non-local: 注意力机制的一种形态](https://zhuanlan.zhihu.com/p/660116933)

[[32] START: 自监督轨迹表示学习框架](https://zhuanlan.zhihu.com/p/660304153)

[[33] 线性复杂度的高效注意力机制：矩阵乘法的交换律](https://zhuanlan.zhihu.com/p/656944206)

[[重读经典, 常读常新][35] 注意力机制在 NLP 领域的诞生](https://zhuanlan.zhihu.com/p/660922705)

[[重读经典，常读常新][36]注意力机制在 NLP 领域的诞生 2](https://zhuanlan.zhihu.com/p/660955612)

[[37] 自注意力机制启蒙：用于句子编码](https://zhuanlan.zhihu.com/p/661013687)

[[重读经典，常读常新][38] Transformer: Attention Is All You Need](https://zhuanlan.zhihu.com/p/661228490)

[[39] LERT: 语言学信息增强的**中文**预训练模型](https://zhuanlan.zhihu.com/p/664200148)

[[40] GAT: GCN + Self-Attention](https://zhuanlan.zhihu.com/p/664316204)

[[41] CA-MSER: 多模态语音情感识别](https://zhuanlan.zhihu.com/p/664350153)

[[42] GraphSAGE: 基于特征的节点向量(图表示)学习方法](https://zhuanlan.zhihu.com/p/664524290)

[[43] Tail-GNN: 提升 graph 中尾节点向量的质量](https://zhuanlan.zhihu.com/p/664631713)

[[44] E-GAN: 用演化算法训练 GAN 模型](https://zhuanlan.zhihu.com/p/664665262)

[[45] HAN: 层级注意力模型用于篇章分类](https://zhuanlan.zhihu.com/p/664713316)

[[46] MO-EGAN: 用多目标演化算法训练GAN](https://zhuanlan.zhihu.com/p/665040587)

[[47] DiffDock:用扩散模型解决分子对接任务](https://zhuanlan.zhihu.com/p/665191390)

[[48] K-EmoCon: 自然对话场景下采集的情感识别数据集](https://zhuanlan.zhihu.com/p/665276718)

[[49] MIND: 一个大规模英文新闻推荐的公开数据集](https://zhuanlan.zhihu.com/p/665453122)

[[50] NRMS: 多头自注意力模型(MHSA)做新闻推荐](https://zhuanlan.zhihu.com/p/666790963)

[[51] EEG Conformer: 融合 CNN 和 Transformer处理 EEG 数据](https://zhuanlan.zhihu.com/p/667108069)

[[52] 多模态方法分析 EEG 和 Language 的关联](https://zhuanlan.zhihu.com/p/667939200)

[[53] LightGCN: 为推荐系统设计的简化线性 GCN](https://zhuanlan.zhihu.com/p/668004690)

[[54] MMGCN: 多模态推荐中建模用户单模态兴趣](https://zhuanlan.zhihu.com/p/668007500)

[[55] GRCN: 计算推荐系统二分图中 u 和 i 的权重](https://zhuanlan.zhihu.com/p/668034694)

[[56] LATTICE: 为多模态推荐系统引入 item-item graph](https://zhuanlan.zhihu.com/p/668077988)

[[57] SGL: 在 GCN 推荐模型中加入自监督学习](https://zhuanlan.zhihu.com/p/668136885)

[[58] MMGCL:多模态图对比学习](https://zhuanlan.zhihu.com/p/668160938)

[[59] SimGCL: 推荐系统中没有图增强的对比学习](https://zhuanlan.zhihu.com/p/668230688)

[[60] NCL: 构建邻居信息进行图对比学习](https://zhuanlan.zhihu.com/p/668291477)

[[61] LightGCL: 使用 SVD 进行图增强](https://zhuanlan.zhihu.com/p/668291997)

[[62] GraphRec: 面向社交网络推荐的 GNN 模型](https://zhuanlan.zhihu.com/p/668599880)

[[63] ProtMD: 利用 MD 数据预训练蛋白质表征模型](https://zhuanlan.zhihu.com/p/668607279)

[[64] DMSGer: 多尺度动态GCN用于高光谱图像分类](https://zhuanlan.zhihu.com/p/668857986)

[[65] SSLRec:自监督推荐模型框架哪家强？](https://zhuanlan.zhihu.com/p/669168807)

[[66] GLUE: 多任务的句子语义理解评估平台](https://zhuanlan.zhihu.com/p/669417513)

[[67] SuperGLUE: 难度升级的 GLUE](https://zhuanlan.zhihu.com/p/669442026)

[[68] ChatGPT vs BERT: 文本语义理解哪家强？](https://zhuanlan.zhihu.com/p/669448809)

[[69] LLMRec: 使用ChatGPT为推荐模型二分图引入先验知识](https://zhuanlan.zhihu.com/p/670111923)

[[70] Gemini 1.0 Google对抗OpenAI的多模态大模型](https://zhuanlan.zhihu.com/p/670817721)





